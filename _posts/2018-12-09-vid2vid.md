---
layout: post
title: vid2vid
categories:
- Research
tags:
- gan
---

[vid2vid](https://arxiv.org/pdf/1808.06601.pdf)提出了一种通用的video to video的生成框架，可以用于很多视频生成任务。常用的pix2pix没有对temporal dynamics建模，所以不能直接用于video synthesis。下面就pose2body对着[vid2vide code](https://github.com/NVIDIA/vid2vid)简单记录一二。

推荐观看vid2vid [youtube](https://www.youtube.com/watch?v=GrP_aOSXt5U&feature=youtu.be)

### 1. Network Architecture

#### 1.1 Sequential Generator

generator在生成当前帧时需要考虑：1）当前帧的输入图片；2）前几帧的输入图片；3）前几帧的生成图片。到底往前回看多少帧由[n_frames_G](https://github.com/NVIDIA/vid2vid/blob/master/options/base_options.py#L58)控制，如下图n_frames_G=3：

![CompositeGenerator](https://raw.githubusercontent.com/7color94/7color94.github.io/master/imgs/vid2vid/CompositeGenerator.png)

经由generator得到当前帧的预测Intermediate image（img_raw）、前一帧到当前帧的光流Flow map（flow）和occlusion Mask（weight_）。当前帧的生成结果最终由前一帧经由光流变化后（img_warp）和当前帧的预测（img_raw）[加权](https://github.com/NVIDIA/vid2vid/blob/master/models/networks.py#L188)而成。

同时对于高分辨率的图片生成，和pix2pixHD一样，提出了coarse-to-fine的[generator](https://github.com/NVIDIA/vid2vid/blob/master/models/networks.py#L201)，由[n_scales_spatial](https://github.com/NVIDIA/vid2vid/blob/master/options/base_options.py#L59)控制：

![CompositeLocalGenerator](https://raw.githubusercontent.com/7color94/7color94.github.io/master/imgs/vid2vid/CompositeLocalGenerator.png)

如果是采用coarse-to-fine generator，在训练时可以通过设置[niter_fix_global](https://github.com/NVIDIA/vid2vid/blob/master/options/train_options.py#L41)让Local G（高分辨率G）单独先训几个epoch，和pix2pix HD中做法类似。

#### 1.2 Image Discriminator

image D的目的是给定原图输入，保证每个生成帧和真实帧保持一致，这和pix2pix HD的D并无差别，结构也采用了Multi-scale PatchGAN。

#### 1.2 Video Discriminator

video D的目的是给定连续的真实帧之间的光流，保证连续的生成帧之间的时间关系（optical flow）和连续的真实帧之间的保持一致。所以image D作用在原图上，video D作用在光流上。

### 2. DataLoader

#### 2.1 PoseDataset

[PoseDataset](https://github.com/NVIDIA/vid2vid/blob/master/data/pose_dataset.py)用来加载openpose、densepose的label和真实的img，同时在训练时做[random scale](https://github.com/NVIDIA/vid2vid/blob/master/data/base_dataset.py)和[random crop](https://github.com/NVIDIA/vid2vid/blob/master/data/base_dataset.py#L93)。

### 3. Training

vid2vid训练时的loss比较多。

### 4. Test