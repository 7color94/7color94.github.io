---
layout: post
title: Cousera Lesson Regression
categories:
- Research
tags:
- machine learning
---

### 一. 前言

这是Cousera上Andrew Ng的机器学习入门公开课，11周的课程马上就要结束了。整体上，我觉得这11周的课程相对比较简单，很容易听懂，少去很多数学的原理推导，很适合初学者，也让初学者建立了机器学习领域的一些宏观概念。

之前，很多人推荐李航博士的《统计学习方法》作为入门教程，看完Andrew Ng的课程视频后，再去阅读《统计学习方法》，会轻松很多

我先总结Week 1~3，主要涉及到监督学习中的线性回归和逻辑回归，避免过拟合等内容

### 二. Week One

#### 2.1 Introduction

##### 2.1.1 环境的建立

教你搭建Octave/MATLAB，这部分不多说

##### 2.1.2 Introduction

说明什么是监督学习，什么是非监督学习

监督学习：

- 对于离散变量，主要是分类问题
- 对于连续变量，主要是回归问题

非监督学习：数据集没有明确的标签，我们需要在这堆数据中寻找他们的数据结构，常见的：聚类问题

#### 2.2 Linear Regression with One Variable

一元线性回归问题

##### 2.2.1 描述一个简单机器学习模型

首先，说明在一个机器学习模型中，要用到的几个字母变量（符号）的定义：

- E: Experience
- T: Tasks
- P: Performance
- x: 特征变量
- y: 目标变量
- m: 训练集的数目
- (x, y): 训练样本
- (x(i), y(i)): 第i个训练样本
- h: 预测函数

好了，有了这些符号，我们就可以表示一个机器学习模型了：

首先，既然是线程回归问题，我们就先假设预测函数h的公式为：

![](http://7xl2fd.com1.z0.glb.clouddn.com/预测函数.png)

其中，

![](http://7xl2fd.com1.z0.glb.clouddn.com/模型参数.png)

我们称为模型参数

然后，学习算法（learning algorithm）会通过training set训练集，得到一个最优的预测函数h（对应一个模型参数）。之后，从测试集中取出x，通过预测函数的映射，即可得到预测的目标变量y

需要说明的是：如何得到一个最优的预测函数h？

很简答，就是通过代价函数，寻找使得代价函数最小的模型参数θ

平方误差是解决回归问题的常用方法，定义一个代价函数J就很简单：

![](http://7xl2fd.com1.z0.glb.clouddn.com/cost-function.png)

通过某种方法，寻找到使得代价函数最小的那个模型参数θ，即是我们最终需要的结果

##### 2.2.2 如何求解模型参数θ（专业地说：如何进行参数学习）

cousera上介绍了一种最经典的算法：**梯度下降算法**

大致的工作过程即，从某个随机的θ0, θ1开始，不停地改变θ0, θ1的值，以保证J(θ0, θ1)不断地减小，直至在某个最小值处结束（不停地在迭代运行）

来张图理解一下（仿佛一个人下山的过程）

![](http://7xl2fd.com1.z0.glb.clouddn.com/gradient%20descent%20img.png)

**梯度下降算法定义：**

![](http://7xl2fd.com1.z0.glb.clouddn.com/gradient%20descent%20define.png)

其中，α叫做学习速率（learning rate），α很大，下山的步伐会迈的很大；α很小，下山的步伐会迈的相对较小。α用以控制我们以多大的幅度去更新参数θj

α不宜设置过大，否则容易出现波动，难以收敛

![](http://7xl2fd.com1.z0.glb.clouddn.com/梯度下降算法中的学习速率.png)

再看看α后面的偏导数项，它代表什么意思呢？

![](http://7xl2fd.com1.z0.glb.clouddn.com/梯度下降算法中的偏导数.png)

先看上面的图，偏导数表示红色线的斜率，此时斜率为正，那么θ1就会减去α*一个正数，θ1会逐渐变小，直至最低点

下面的图，斜率为负，θ1会逐渐变大，直至最低点

ok，梯度下降算法定义解释完毕。在本例中，hθ(x)=θ0+θ1*x，那么梯度下降算法的具体步骤为：

![](http://7xl2fd.com1.z0.glb.clouddn.com/一元线性回归的梯度下降算法.png)

此时，已经具体到了θ0和θ1的具体变换过程

需要注意的是：θ0和θ1是需要同时更新变化执行迭代的


### 三. Week Two

#### 3.1 Linear Regression with Multiple Variables 

多元线性回归问题

既然已经不止一个特征变量了，那么之前用x表示特征变量，现在要换一种表示方法了：

- n: 表示特征值的个数
- xj(i)：第i个训练样本的第j个特征的值

此时预测函数h表示为：hθ(x) = θT*x

为了符号的方便表示，我们总是定义x(j)0为1

![](http://7xl2fd.com1.z0.glb.clouddn.com/多元线性回归的预测函数.png)

下面，解释在多元线性回归中的梯度下降算法：

首先，是它的代价函数

![](http://7xl2fd.com1.z0.glb.clouddn.com/多元线性回归的代价函数.png)

然后是θj的变换过程

![](http://7xl2fd.com1.z0.glb.clouddn.com/多元线性回归的梯度下降算法.png)

ok，下面再介绍梯度下降算法在实际应用中的两个注意点：

- 特征缩放
- 学习速率的选择

**特征缩放：**

假设，现有一个房价预测系统，若现有两个特征：x1, x2。x1表示房屋的面积，取值范围：0~2000，x2表示房间的个数，取值范围：1~5

这两个特征的取值的取值规模不在一个级别，使用梯度下降算法时，收敛不快，且容易发生波动。应使用特征缩放方法将两个特征的取值范围约束到大体一致的范围：

- 一种方法是：x1=x1/2000，每个特征处以该特征值的最大值。这样，取值范围都被缩小为0~1。一般-1~1, -1/3~1/3和-3~3均可接受
- 第二种方法，归一化处理：（xi-平均值）/（MAX-MIN）

**学习速率α**

上面已经解释了learning rate的意义。正常的梯度下降算法，随着迭代次数的增加，代价函数逐渐减小；不正常的工作流程，有可能出现波动现象

![](http://7xl2fd.com1.z0.glb.clouddn.com/梯度下降算法的迭代过程.png)

不正常的工作现象：

![](http://7xl2fd.com1.z0.glb.clouddn.com/不正常的梯度下降过程.png)

看了公开课后的结论是：α一般从0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1开始选择

截至目前，我们一直都在使用梯度下降算法求解最优的模型参数。梯度下降算法最显著的特征就是，需要不停地迭代

视频中还介绍了另一种求解方法：**正规方程（Normal Equation）**

θ=(xT*x)-1*xT*y（证明略...）

它一步即可得到最优的模型参数值

现在，我们来比较一下：

<table border="1">
<tr>
<th>梯度下降算法（Gradient Descent）</th>
<th>正规方程（Normal Equation）</th>
</tr>
<tr>
<td>需要选择学习速率α</td>
<td>不需要选择学习速率α</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>不需要迭代，一步得结果</td>
</tr>
<tr>
<td>当特征数目n特别大时，也可以正常工作</td>
<td>当特征数目n特别大时，矩阵纬度很大，计算非常慢（n=100,1000还好，n为千万时，计算非常慢）</td>
</tr>
</table>

### 四. Week Three 逻辑回归

#### 4.1 分类问题

先从两元分类问题进行研究：

y∈{0, 1}，定义y=0时，属于Negtive Class；y＝1时，属于Positive Class

那么，怎样让预测函数 0 <= hθ(x) <= 1？

这里介绍S型函数g(z)=1/(1 + e-z)

![](http://7xl2fd.com1.z0.glb.clouddn.com/S型函数.png)

S型函数的取值范围在(0, 1)，则hθ(x) = 1/（1 + e-(θT*x)）；而且hθ(x) = P(y=1 | x;θ) = 1 - P(y=0 | x;θ)

- 当θT*x >= 0，hθ(x) >= 0.5，y为1
- 当θT*x < 0，hθ(x) < 0.5，y为0

举例：hθ(x) = -3 + x1 + x2；当-3 + x1 + x2 >= 0，预测y=1；此时决策边界为 x1 + x2 = 3

需要注意的是：我们不是用训练集来定义决策边界的，而是用来拟合参数θ的，θ决定了决策边界

#### 4.2 代价函数和梯度下降算法

之前在线性回归中定义的平方误差代价函数，是否能用到逻辑回归中呢？

答案是：不能

因为逻辑回归中的预测函数hθ(x)是非线性函数，得到的代价函数将是非凸函数，可能是这样：

![](http://7xl2fd.com1.z0.glb.clouddn.com/非凸代价函数.png)

它有许多局部最优值，梯度下降算法不能保证收敛到全局最优值。所以，我们希望代价函数是有类似于“单弓形”函数，能确保找到全局最小值

我们需要重新定义逻辑回归的代价函数了

![](http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归的代价函数.png)

当 y = 1，hθ(x) -> 0, cost -> ∞

![](http://7xl2fd.com1.z0.glb.clouddn.com/y=1的逻辑回归代价函数.png)

当 y = 0，hθ(x) -> 1, cost -> ∞

![](http://7xl2fd.com1.z0.glb.clouddn.com/y=0的逻辑回归代价函数.png)

ok，新定义的代价函数，符合我们的要求

此时的代价函数公式，用一行表示为：

![](http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归代价函数总式子.png)

使用梯度下降算法，迭代并同时更新θ0～θn

![](http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归梯度下降算法.png)

和线性回归的一样，只不过hθ(x)的表达式不同

当然，特征缩放也是适用于逻辑回归的

#### 4.3 高级算法

除了梯度下降，正规方程，还有许多可以计算出最优模型参数θ的算法

视频中还提到了：BFGS, L-BFGS和Conjugate gradient

这些高级算法比梯度下降要快，无需选择学习速率，但是更复杂。具体编程时，可以考虑使用现成的类库，帮助我们快速解决问题

#### 4.4 多元分类问题

三元分类问题

可以转换为三个两元分类问题来做

hθ(i)(x) = P(y = i | x:θ)，其中 i＝1，2，3

训练的出θ后，预测时，选择hθ(i)(x)最大的那个i，就是目标变量的分类

#### 4.5 过拟合问题

##### 4.5.1 什么是过拟合？什么是欠拟合？

- 欠拟合：预测模型没有能很好的拟合训练数据，拟合效果差。产生高偏差
- 过拟合：能很好地拟合训练数据。但预测函数变量过多，而没有足够多的数据去约束变量的个数（预测测试集，效果会非常差，即泛化效果差）。产生高方差

[高偏差；高仿差..click for infos](http://www.zhihu.com/question/20448464)

＊泛化：一个测试模型应用到新样本的能力

这里，先不讲如何识别过拟合问题，后面的课程讲解了如果利用工具去识别过拟合问题（后续整理）

往往，我们往预测模型中，增加了太多太多的特征，容易导致过拟合问题

如何避免过拟合问题：

- 减少特征变量数目（缺点：同时也会丢失一些重要信息）
- 正则化（进行某种惩罚措施）：保留所有的特征变量，但是降低模型参数θj的数量级或者值

正则化技术非常有效，由于是在我们预测模型中有许多特征变量，每个特征变量都能帮助我们预测目标变量，这样我们就可以不用舍弃任何一个特征变量了

##### 4.5.2 线性回归代价函数中使用正则化技术：

重写线性回归中的代价函数：

![](http://7xl2fd.com1.z0.glb.clouddn.com/正则化的线性回归代价函数.png)

λ为正则化参数。λ越大，惩罚程度越大，模型参数θj会越小，有可能会导致欠拟合问题

##### 4.5.3 线性回归梯度下降算法中使用正则化技术：

![](http://7xl2fd.com1.z0.glb.clouddn.com/正则化的梯度下降算法.png)

注意θ0是单独的一种

##### 4.5.4 正规方程中使用正则化技术：

![](http://7xl2fd.com1.z0.glb.clouddn.com/正则化的正规方程.png)

##### 4.5.5 逻辑回归代价函数中使用正则化技术：

![](http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归正则化代价函数.png)

##### 4.5.6 逻辑回归梯度下降算法中使用正则化技术：

![](http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归正则化梯度下降算法.png)

这个和线性回归中的一样，hθ(x)不同而已