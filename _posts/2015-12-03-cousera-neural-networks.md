---
layout: post
title: Cousera Lesson Neural Network
categories:
- Research
tags:
- machine learning
---

### 一. 神经网络模型

神经网络可以表达一个复杂且非线性的假设模型，其模型表示（图片见本节最后边）：

最左边Layer 1称为输入层；Layer 2称为隐藏层；隐藏层每一个元素也叫做“神经元”，它是由前一层通过“激励函数”计算得来的；最后Layer 3称为输出层；该模型隐藏层比较少，绝大多数的数量不止一个

其中：ai(j)表示第j层的第i个神经元，也称为激励值；激励函数可以选择g(z)=1/(1 + e-z) 

备注：激励函数是对类似的非线性函数g(z)的另一种称呼而已

那么：

- a1(2) = g(Θ10(1)X0 + Θ11(1)x1 + Θ12(1)x2 + Θ13(1)x3)
- a2(2) = g(Θ20(1)x0 + Θ21(1)x1 + Θ22(1)x2 + Θ23(1)x3)
- a3(2) = g(Θ20(1)x0 + Θ31(1)x1 + Θ32(1)x2 + Θ33(1)x3)
- hΘ(x) = a1(3) = g(Θ10(2)a0(2) + Θ11(2)a1(2) + Θ12(2)a2(2) + Θ13(2)a3(2))

其中 Θij(l) 是 第l层 第j个单元 与 第l+1层第i个单元之间的联接参数（其实就是连接线上的权重，注意标号顺序）

ok，一个简单的模型表示出来了，其实还可以精简一点表示：

重新定义如下：

- a1(2) = g(z1(2))
- a2(2) = g(z2(2))
- a3(2) = g(z3(2))
- hΘ(x) = a1(3) = g(z1(3))
- z(2) = Θ(1)X

其中，z是x0, x1, x2, x3的加权线性组合

言而总之，我们从输入层的激励开始，然后进行**前向传播**给隐藏层计算，并进行隐藏层的激励，然后继续传播，计算输出层的激励

所以，神经网络的工作原理是：

其本质做的就是逻辑回归，但不直接用x1, x2, x3作为输入特征，而是用a1, a2, a3，而a1, a2, a3是由Θ(1)所决定的。即不直接使用输入特征来训练逻辑回归，而是自己训练逻辑回归的输入特征a1, a2, a3

Θ(1)不同，会训练出不同复杂而有趣的特征

![](http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-model.png)

### 二. 神经网络的应用

#### 2.1 具体例子

使用神经网络实现x1 AND x2

![](http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-AND.png)

相应的，可以实现x1 OR x2; x1 XOR x2; x1 XNOR x2。只是，各自的参数权重不同而已

#### 2.2 多类型分类系统

![](http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-app.png)

预测的输出值有多个，可以预测多种类型

### 三. 反向传播算法

神经网络的模型已经被我们表示出来了，那么我们如何学习模型参数？利用反向传播算法

我们要先找到神经网络的代价函数，目的是min它

#### 3.1 神经网络的代价函数

先定义一些字母变量

- L：神经网络中的层数
- Sl：第l层神经单元的个数
- k：输出单元数量；显然，Binary Classification的k为1，multi-class Classification的k就是k，k维向量

既然神经网络的本质就是逻辑回归，我们先来看看逻辑回归的代价函数：

![](http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归代价函数总式子.png)

只不过，在神经网络中，输出单元可能不止一个，那么神经网络的代价函数为：

![](http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-cost-function.png)

#### 3.2 反向传播算法

为了计算导数项，我们将使用反向传播算法

反向传播算法某种意义上是对每一个结点计算这样的一项：δj(l)，即第l层第j个结点的误差

具体计算方法：

![](http://7xl2fd.com1.z0.glb.clouddn.com/反向传播算法-1.png)

让我们来整合一下，当我们拥有大量训练样本的时候，如何实现反向传播算法：

![](http://7xl2fd.com1.z0.glb.clouddn.com/反向传播算法-2.png)

好了，仔细看看上面两张图片，我们的最终目的是计算出了偏导数项。其实就是计算出梯度下降的方向

PS: 我操，步骤好多。参数好多。。

### 四. 实际中如何使用反向传播算法

#### 4.1 梯度检查(Gradient Checking)

梯度检查，是为了避免反向传播算法实现过程中的小错误；当出现错误时，J(Θ)看似在减小，实则最终值会比没有错误时要高

其核心原理是J(θ)在θ处的偏导数可以约等于(J(θ+ε) - J(θ-ε)) / 2ε，其中ε要足够小，一般去10的-4次方：

![](http://7xl2fd.com1.z0.glb.clouddn.com/梯度检查原理.png)

这是高中就知道的知识点了，具体应用到梯度检查中就是：

![](http://7xl2fd.com1.z0.glb.clouddn.com/梯度检查算法)

上面图中，我们可以for循环1~n来计算代价函数在每个网络中的参数的偏导数的近似值，记为gradApprox

我们把反向传播算法计算得到的梯度记为DVec

这样，我们比较gradApprox和DVec是否近似相等（一般最多几位小数的差距）

ok，总结一下就是：

- 先实现反向传播算法，计算出DVec
- 实现数值梯度检查算法，计算出gradApprox
- 确定两者给出的值，是接近的
- 关闭梯度检查（梯度检查计算量相当大），使用反向传播进行神经网络的学习

所以，梯度检查是是用来验证你的方向传播的，不是用来学习参数的

#### 4.2 如何随机初始化Θ？

是否可以将Θ的每个参数都初始化为0？不可以！

![](http://7xl2fd.com1.z0.glb.clouddn.com/神经网络初始化参数不能为0.png)

这会导致a1(2)永远等于a2(2)；所有的隐藏单元都会通过完全相同的输入函数计算出来，这完全是多余的

正确的姿势是：打破这种对称的权重

随机初始化Θij(l)在[-ε, +ε]之间即可，之间的任何一个数

### 五. 总结回顾

神经网络的知识点还是比较复杂，需要来一波总结

#### 5.1 如何选择神经网络的框架

该选择多少个隐藏层？每个隐藏层拥有多少个神经元？

一般默认

- 一个隐藏层
- 或者多于一个隐藏层，每个隐藏层应具备同样数目的神经元（神经元数量并不是越多越好，越多，计算量也越大），最好能和输入特征的维度匹配

比如：3，5，4 或 3，5，5，4 或 3，5，5，5，4

#### 5.2 总结神经网络训练模型的步骤

1.随机初始化权重（即参数）
2.对于训练集中的每一个x(i)，使用前向传播算法得到hΘ(x(i))
3.计算代价函数J(Θ)
4.实现反向传播算法，计算J(Θ)在Θj处的偏导数
5.使用梯度检查算法，确保反向传播算法无差错工作；之后，关闭该算法
6.使用梯度下降算法或者其余高级算法（结合反向传播算法），minmizeJ(Θ)，得到最优的模型参数Θ

反向传播的目的是为了计算梯度下降的方向