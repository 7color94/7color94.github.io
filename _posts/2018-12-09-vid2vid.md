---
layout: post
title: vid2vid
categories:
- Research
tags:
- gan
---

[vid2vid](https://arxiv.org/pdf/1808.06601.pdf)提出了一种通用的video to video的生成框架，可以用于很多视频生成任务。常用的pix2pix没有对temporal dynamics建模，所以不能直接用于video synthesis。下面就pose2body对着vid2vide [code](https://github.com/NVIDIA/vid2vid)简单记录一二。

推荐观看vid2vid [youtube](https://www.youtube.com/watch?v=GrP_aOSXt5U&feature=youtu.be)

### 1. Network Architecture

#### 1.1 Sequential Generator

sequential generator在生成当前帧时需要考虑：1）当前帧的输入图片；2）前几帧的输入图片；3）前几帧的生成图片。回看多少帧由[n_frames_G](https://github.com/NVIDIA/vid2vid/blob/master/options/base_options.py#L58)设置，如下图中n_frames_G设置为3：

![CompositeGenerator](https://raw.githubusercontent.com/7color94/7color94.github.io/master/imgs/vid2vid/CompositeGenerator.png)

sequential generator可以得到当前帧的预测图intermediate image、前一帧到当前帧的光流图flow map和occlusion mask。之后当前帧的生成结果由前一帧经由光流变化后的图片（img_warp）和当前帧的预测图（img_raw）通过mask [加权](https://github.com/NVIDIA/vid2vid/blob/master/models/networks.py#L188)而成，这样做是因为前后连续帧间有非常多的相似信息。

同时对于高分辨率的图片生成，和pix2pixHD一样，提出了coarse-to-fine的[generator](https://github.com/NVIDIA/vid2vid/blob/master/models/networks.py#L201)，通过[n_scales_spatial](https://github.com/NVIDIA/vid2vid/blob/master/options/base_options.py#L59)控制：

![CompositeLocalGenerator](https://raw.githubusercontent.com/7color94/7color94.github.io/master/imgs/vid2vid/CompositeLocalGenerator.png)

如果是采用coarse-to-fine generator，在训练时可以通过设置[niter_fix_global](https://github.com/NVIDIA/vid2vid/blob/master/options/train_options.py#L41)让Local 部分（高分辨率部分）单独先训几个epoch，和pix2pixHD中做法类似。

#### 1.2 Image Discriminator

image discriminator的目的是给定相同的图片输入，让生成帧和真实帧保持一致。这和pix2pixHD的discriminator并无差别，结构也采用了Multi-scale PatchGAN。

![ImageDiscriminator](https://raw.githubusercontent.com/7color94/7color94.github.io/master/imgs/vid2vid/ImageDiscriminator.png)

#### 1.3 Video Discriminator

video discriminator的目的是给定相同的光流，让连续的生成帧之间的光流信息和连续的真实帧之间的保持一致。所以image discriminator作用在图片上，video discriminator作用在光流上。

另外，vid2vid设计了temporally multi-scale video discriminator。这里“multi-scale”目的是采样连续帧的密度由紧密到稀疏，以便捕捉short-term和long-term的光流一致性，具体采用细节在代码[get_skipped_frames](https://github.com/NVIDIA/vid2vid/blob/master/train.py#L273)中体现。每个“scale”（密度）的video discriminator都是一个Multi-scale PatchGAN。

![VideoDiscriminator](https://raw.githubusercontent.com/7color94/7color94.github.io/master/imgs/vid2vid/VideoDiscriminator.png)

### 2. DataLoader

#### 2.1 PoseDataset

[PoseDataset](https://github.com/NVIDIA/vid2vid/blob/master/data/pose_dataset.py)用来加载openpose、densepose的label和真实的img，同时在训练时做[random scale](https://github.com/NVIDIA/vid2vid/blob/master/data/base_dataset.py)和[random crop](https://github.com/NVIDIA/vid2vid/blob/master/data/base_dataset.py#L93)。

### 3. Training

vid2vid训练时的[loss](https://github.com/NVIDIA/vid2vid/blob/master/train.py#L143)比较多。

- 1) [D_fake，D_real，G_GAN，G_GAN_Feat](https://github.com/NVIDIA/vid2vid/blob/master/models/vid2vid_model_D.py#L185)，[G_VGG](https://github.com/NVIDIA/vid2vid/blob/master/models/vid2vid_model_D.py#L184)：作用在图片上的基本loss，和pix2pixHD一样
- 3) [F_Flow](https://github.com/NVIDIA/vid2vid/blob/master/models/vid2vid_model_D.py#L169)：sequential generator生成的光流flow通过MaskedL1Loss约束，要和真实的光流flow_ref尽可能相似
- 2) [G_Warp](https://github.com/NVIDIA/vid2vid/blob/master/models/vid2vid_model_D.py#L188)：生成的前一帧fake_B_prev通过真实的光流flow_ref warp得到的fake_B_warp_ref要和生成当前帧fake_B尽可能相似
- 4) [F_Warp](https://github.com/NVIDIA/vid2vid/blob/master/models/vid2vid_model_D.py#L172)：真实的前一帧real_B_prev通过generator生成的光流flow warp得到的real_B_warp要和真实当前帧real_B尽可能相似
- 5) [W](https://github.com/NVIDIA/vid2vid/blob/master/models/vid2vid_model_D.py#L178)：让generator生成的mask和全0的tensor尽可能相似？？
- 6) [G_f_GAN，G_f_GAN_Feat，D_f_fake，D_f_real](https://github.com/NVIDIA/vid2vid/blob/master/models/vid2vid_model_D.py#L201)：FaceGAN的loss，对face region的精细优化
- 7) [G_T_GAN，G_T_GAN_Feat，G_T_Warp，D_T_fake，D_T_real](https://github.com/NVIDIA/vid2vid/blob/master/models/vid2vid_model_D.py#L154)：temporal loss，作用在采样得到的一段连续帧上的loss，T表示不同采用密度

### 4. Test

利用sequential generator不断生成就可以了。